# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ik70sSt5W0d5s3UbiKkYgtUAQghY7t1n
"""

#Description : This is a sentiment analysis program that parses the tweets fetched from Twitter using Python

# Commented out IPython magic to ensure Python compatibility.
#Import the libraries
import tweepy
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

import pandas as pd
import numpy as np
import nltk
import string
import fasttext
import contractions
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
plt.xticks(rotation=70)
pd.options.mode.chained_assignment = None
pd.set_option('display.max_colwidth', 100)
# %matplotlib inline

#nltk package
!pip install fasttext
!pip install Contractions

#Load the data
from google.colab import files
uploaded=files.upload()

import io

covidvac_df=pd.read_csv(io.StringIO(uploaded['covidvac.csv'].decode('utf-8')))

covidvac_df.head()

#Text Pre-Processing
# 1.Expanding Contractions
covidvac_df['no_contract'] = covidvac_df['Tweets'].apply(lambda x: [contractions.fix(word) for word in x.split()])
covidvac_df.head()

# 2. Expanding Contractions
covidvac_df['Tweets_str'] = [' '.join(map(str, l)) for l in covidvac_df['no_contract']]
covidvac_df.head()

#Tokenization
import nltk
nltk.download('punkt')
covidvac_df['tokenized'] = covidvac_df['Tweets'].apply(word_tokenize)
covidvac_df.head()

#Converting all Characters to Lowercase
covidvac_df['lower'] = covidvac_df['tokenized'].apply(lambda x: [word.lower() for word in x])
covidvac_df.head()

#Removing Punctuations
punc = string.punctuation
covidvac_df['no_punc'] = covidvac_df['lower'].apply(lambda x: [word for word in x if word not in punc])
covidvac_df.head()

#Removing Stopwords
import nltk
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
covidvac_df['stopwords_removed'] = covidvac_df['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])
covidvac_df.head()

#1.Stemming vs Lemmatization
import nltk
nltk.download('averaged_perceptron_tagger')
covidvac_df['pos_tags'] = covidvac_df['stopwords_removed'].apply(nltk.tag.pos_tag)
covidvac_df.head()

#2.Stemming vs Lemmatization
import nltk
nltk.download('wordnet')
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
covidvac_df['wordnet_pos'] = covidvac_df['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])
covidvac_df.head()

#3.Stemming vs Lemmatization
wnl = WordNetLemmatizer()
covidvac_df['lemmatized'] = covidvac_df['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])
covidvac_df.head()

covidvac_df.to_csv('covidvac_clean.csv')

#Load the data
from google.colab import files
uploaded=files.upload()

import io

covidvac_clean_df=pd.read_csv(io.StringIO(uploaded['covidvac_clean.csv'].decode('utf-8')))

covidvac_clean_df.head()

#create a function to get the subjectivity
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

#create a function to get the polarity
def getPolarity(text):
  return TextBlob(text).sentiment.polarity
#create two new columns
covidvac_clean_df['Subjectivity']=covidvac_clean_df['cleaned_text'].apply(getSubjectivity)
covidvac_clean_df['Polarity']=covidvac_clean_df['cleaned_text'].apply(getPolarity)
#show the new dataframe with the new columns
covidvac_clean_df

#Create a function to compute the negative,neutral and positive analysis
def getAnalysis(score):
  if score<0:
    return 'Negative'
  elif score==0:
      return 'Neutral'
  else:
    return 'Positive'
  
covidvac_clean_df['Analysis']=covidvac_clean_df['Polarity'].apply(getAnalysis)

#Show the dataframe
covidvac_clean_df

#Print all of the positive tweets
j=1
sortedDF=covidvac_clean_df.sort_values(by=['Polarity'])
for i in range(0,sortedDF.shape[0]):
  if sortedDF['Analysis'][i]=='Positive':
   print(str(j)+ ')'+sortedDF['cleaned_text'][i])
   print()
  j=j+1

#Print all of the negative tweets
j=1
sortedDF=covidvac_clean_df.sort_values(by=['Polarity'])
for i in range(0,sortedDF.shape[0]):
  if sortedDF['Analysis'][i]=='Negative':
    print(str(j)+ ') '+sortedDF['cleaned_text'][i])
    print()
    j=j+1

#Print all of the neutral tweets
j=1
sortedDF=covidvac_clean_df.sort_values(by=['Polarity'])
for i in range(0,sortedDF.shape[0]):
  if sortedDF['Analysis'][i]=='Neutral':
    print(str(j)+ ') '+sortedDF['cleaned_text'][i])
    print()
    j=j+1

#Get the percentage of positive tweets
ptweets=covidvac_clean_df[covidvac_clean_df.Analysis=='Positive']
ptweets=ptweets['cleaned_text']
round( (ptweets.shape[0] / covidvac_clean_df.shape[0])*100,1)

#Get the percentage of negative tweets
ntweets=covidvac_clean_df[covidvac_clean_df.Analysis=='Negative']
ntweets=ntweets['cleaned_text']
round( (ntweets.shape[0] / covidvac_clean_df.shape[0])*100,1)

#Get the percentage of neutral tweets
netweets=covidvac_clean_df[covidvac_clean_df.Analysis=='Neutral']
netweets=netweets['cleaned_text']
round( (netweets.shape[0] / covidvac_clean_df.shape[0])*100,1)

#Show the value counts
covidvac_clean_df['Analysis'].value_counts()

#plot and visualize the counts
plt.title('Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
covidvac_clean_df['Analysis'].value_counts().plot(kind='bar')
plt.show()